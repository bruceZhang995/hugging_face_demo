# å®‰è£…æ ¸å¿ƒåº“ï¼ˆæœ€æ–°ç‰ˆï¼‰
# !pip install transformers gradio torch
import os
os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'  # ç¦ç”¨ç¬¦å·é“¾æ¥è­¦å‘Š
import gradio as gr
from transformers import pipeline
from opencc import OpenCC  # ç”¨äºç®€ç¹è½¬æ¢
from transformers import pipeline, GPT2Tokenizer, GPT2LMHeadModel, BertTokenizer, AutoModelForCausalLM,AutoTokenizer

### pipelineæ”¯æŒçš„task
"""
"audio-classification": will return a [`AudioClassificationPipeline`].
"automatic-speech-recognition": will return a [`AutomaticSpeechRecognitionPipeline`].
"conversational": will return a [`ConversationalPipeline`].
"depth-estimation": will return a [`DepthEstimationPipeline`].
"document-question-answering": will return a [`DocumentQuestionAnsweringPipeline`].
"feature-extraction": will return a [`FeatureExtractionPipeline`].
"fill-mask": will return a [`FillMaskPipeline`]:.
"image-classification": will return a [`ImageClassificationPipeline`].
"image-feature-extraction": will return an [`ImageFeatureExtractionPipeline`].
"image-segmentation": will return a [`ImageSegmentationPipeline`].
"image-to-image": will return a [`ImageToImagePipeline`].
"image-to-text": will return a [`ImageToTextPipeline`].
"mask-generation": will return a [`MaskGenerationPipeline`].
"object-detection": will return a [`ObjectDetectionPipeline`].
"question-answering": will return a [`QuestionAnsweringPipeline`].
"summarization": will return a [`SummarizationPipeline`].
"table-question-answering": will return a [`TableQuestionAnsweringPipeline`].
"text2text-generation": will return a [`Text2TextGenerationPipeline`].
"text-classification"` (alias `"sentiment-analysis"` available): will return a [`TextClassificationPipeline`].
"text-generation": will return a [`TextGenerationPipeline`]:.
"text-to-audio"` (alias `"text-to-speech"` available): will return a [`TextToAudioPipeline`]:.
"token-classification"` (alias `"ner"` available): will return a [`TokenClassificationPipeline`].
"translation": will return a [`TranslationPipeline`].
"translation_xx_to_yy": will return a [`TranslationPipeline`].
"video-classification": will return a [`VideoClassificationPipeline`].
"visual-question-answering": will return a [`VisualQuestionAnsweringPipeline`].
"zero-shot-classification": will return a [`ZeroShotClassificationPipeline`].
"zero-shot-image-classification": will return a [`ZeroShotImageClassificationPipeline`].
"zero-shot-audio-classification": will return a [`ZeroShotAudioClassificationPipeline`].
"zero-shot-object-detection": will return a [`ZeroShotObjectDetectionPipeline`].
"""

# ç®€ç¹è½¬æ¢å™¨
cc = OpenCC('t2s')  # ç¹ä½“è½¬ç®€ä½“

# åˆ›å»ºç”Ÿæˆå™¨ï¼ˆæ·»åŠ æ›´å®‰å…¨çš„æ¨¡å‹åŠ è½½ï¼‰
try:
    model_path = r"./models/gpt2-chinese-cluecorpussmall"

    model = AutoModelForCausalLM.from_pretrained(model_path)
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    # generator = pipeline('text-generation', model='gpt2', device=0)
    generator = pipeline('text-generation', model=model,tokenizer=tokenizer, device=0)
    print("åŠ è½½æˆåŠŸ!")

except:
    # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨æ›´å°æ¨¡å‹
    generator = pipeline('text-generation', model='distilgpt2', device=0)

    try:
        # å¤‡ç”¨ä¸­æ–‡æ¨¡å‹
        generator = pipeline('text-generation',
                            model='IDEA-CCNL/Wenzhong-GPT2-110M',
                            device=0)
    except:
        # æœ€åå°è¯•è‹±æ–‡æ¨¡å‹
        generator = pipeline('text-generation', model='gpt2', device=0)

def generate_text(prompt, max_length=150, temperature=0.7, top_p=0.9, repetition_penalty=1.5):
    """
        temperature         0.5 - 0.9       å€¼è¶Šé«˜è¶Šå¤©é©¬è¡Œç©ºï¼Œå€¼è¶Šä½è¶Šä¿å®ˆ
        top_p               0.85 - 0.95     åªè€ƒè™‘æ¦‚ç‡ç´¯ç§¯è¾¾é˜ˆå€¼çš„è¯æ±‡
        repetition_penalty  1.2 - 2.0       æœ‰æ•ˆé¿å…é‡å¤çŸ­è¯­
        max_length          80 - 200        æ ¹æ®éœ€æ±‚è°ƒæ•´è¾“å‡ºé•¿åº¦
    """
    params = {
        "max_length": int(max_length),  # é•¿åº¦ï¼ˆ50-200ä¹‹é—´ï¼‰
        "num_return_sequences":1,       # å›ºå®šç”Ÿæˆ1ä¸ªç»“æœ
        "temperature": float(temperature),      # æ§åˆ¶éšæœºæ€§ï¼š0.3-0.9ï¼ˆå€¼è¶Šä½è¶Šä¿å®ˆï¼‰
        "top_k": 50,            # æ ¸é‡‡æ ·ï¼ˆè¿‡æ»¤ä½æ¦‚ç‡è¯ï¼‰; å›ºå®šå€¼
        "top_p": float(top_p),  # é™åˆ¶å€™é€‰è¯æ•°é‡
        "repetition_penalty": float(repetition_penalty), # æŠ‘åˆ¶é‡å¤ï¼ˆ>1.0ç”Ÿæ•ˆï¼‰
        "do_sample":True,       # å¿…é¡»å¼€å¯é‡‡æ ·æ¨¡å¼
    }
    try:
        result = generator(prompt, **params)[0]['generated_text']
        # ç®€ç¹è½¬æ¢ï¼ˆç¡®ä¿è¾“å‡ºç®€ä½“ä¸­æ–‡ï¼‰
        simplified_result = cc.convert(result)
        return simplified_result
    except Exception as e:
        return f"ç”Ÿæˆå¤±è´¥: {str(e)}"
    return result

def main():

    # ä½¿ç”¨Blocksåˆ›å»ºæ›´å¤æ‚çš„ç•Œé¢
    with gr.Blocks(title="AIæ–‡æœ¬ç”Ÿæˆå®éªŒå®¤") as demo:
        gr.Markdown("## ğŸ® æ–‡æœ¬ç”Ÿæˆå‚æ•°è°ƒèŠ‚å°")

        with gr.Row():
            # å·¦ä¾§ï¼šè¾“å…¥å’Œæ§åˆ¶åŒº
            with gr.Column():
                prompt_input = gr.Textbox(
                    label="è¾“å…¥æç¤º",
                    placeholder="åœ¨è¿™é‡Œè¾“å…¥ä½ çš„åˆ›æ„å¼€å¤´...",
                    lines=3
                )

                with gr.Accordion("é«˜çº§å‚æ•°", open=False):
                    max_length_slider = gr.Slider(
                        minimum=50, maximum=300, value=100, step=10,
                        label="ç”Ÿæˆé•¿åº¦", interactive=True
                    )

                    temperature_slider = gr.Slider(
                        minimum=0.1, maximum=1.0, value=0.7, step=0.1,
                        label="éšæœºæ€§ (temperature)", interactive=True
                    )

                    top_p_slider = gr.Slider(
                        minimum=0.5, maximum=1.0, value=0.9, step=0.05,
                        label="å¤šæ ·æ€§ (top-p)", interactive=True
                    )

                    repetition_penalty_slider = gr.Slider(
                        minimum=1.0, maximum=2.0, value=1.2, step=0.1,
                        label="é˜²é‡å¤æƒ©ç½š", interactive=True
                    )

                generate_btn = gr.Button("ç”Ÿæˆæ–‡æœ¬", variant="primary")

            # å³ä¾§ï¼šè¾“å‡ºåŒº
            with gr.Column():
                output_text = gr.Textbox(
                    label="ç”Ÿæˆç»“æœï¼ˆç®€ä½“ä¸­æ–‡ï¼‰",
                    interactive=False,
                    lines=10,
                    show_copy_button=True
                )

        # æ·»åŠ ç¤ºä¾‹æç¤º
        gr.Examples(
            examples=[
                ["æœªæ¥ä¸–ç•Œï¼Œæœºå™¨äººæ‹¥æœ‰äº†æƒ…æ„Ÿï¼Œä»–ä»¬"],
                ["åœ¨ä¸€ä¸ªé­”æ³•ç‹å›½é‡Œï¼Œä¼šè¯´è¯çš„çŒ«"],
                ["å¦‚æœæ—¶é—´æ—…è¡Œæˆä¸ºç°å®ï¼Œå†å²å­¦å®¶ä¼š"]
            ],
            inputs=prompt_input,
            label="è¯•è¯•è¿™äº›ä¸­æ–‡ä¾‹å­"
        )

        # ç»‘å®šæŒ‰é’®äº‹ä»¶ - åªä¼ é€’ç•Œé¢æœ‰çš„å‚æ•°
        generate_btn.click(
            fn=generate_text,
            inputs=[
                prompt_input,
                max_length_slider,
                temperature_slider,
                top_p_slider,
                repetition_penalty_slider
            ],
            outputs=output_text
        )

    # å®‰å…¨å¯åŠ¨ï¼ˆè‡ªåŠ¨ç«¯å£æ£€æµ‹ï¼‰
    port = 7860
    while port < 8000:
        try:
            demo.launch(server_name="127.0.0.1", server_port=port)
            print(f"æˆåŠŸå¯åŠ¨åœ¨ç«¯å£: {port}")
            break
        except:
            print(f"ç«¯å£ {port} è¢«å ç”¨ï¼Œå°è¯•ä¸‹ä¸€ä¸ª...")
            port += 1
    return

if __name__ == '__main__':
    main()